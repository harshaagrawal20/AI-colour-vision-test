# üé® Color Vision AI Testing System

An **AI-powered, dynamic color vision testing system** that generates customized color accuracy tests from any uploaded image. Unlike traditional fixed color blindness tests (Ishihara, D-15), this system intelligently extracts dominant colors, creates personalized tests, and classifies potential color vision deficiencies.

## üöÄ Features

‚ú® **Dynamic Test Generation**
- Extracts dominant colors from user-uploaded images using K-Means clustering
- Converts colors to CIE LAB color space for accurate perception analysis
- Generates customized patch-ordering or matching tests

üìä **AI-Powered Classification**
- Analyzes error patterns to detect:
  - **Normal Vision**
  - **Protan Deficiency** (Red-Green, Red end)
  - **Deutan Deficiency** (Red-Green, Green end)
  - **Tritan Deficiency** (Yellow-Blue)
  - **Display Calibration Issues**
- Computes confidence scores and severity levels

üéØ **Comprehensive Error Analysis**
- Computes CIE ŒîE (color difference) in LAB space
- Analyzes errors across L, a, b channels
- Maps confusion axes to deficiency types
- Generates actionable recommendations

üíª **User-Friendly Interface**
- Drag-and-drop color arrangement
- Real-time feedback
- Visual accuracy score (0-100%)
- Detailed diagnostic reports

## üìã System Architecture

### Backend (FastAPI + Python)
- **Color Extraction** (`color_extractor.py`): RGB ‚Üí LAB conversion, K-Means clustering
- **Test Generation** (`test_generator.py`): Creates patch tests, distractor tests, luminance-equalized tests
- **Error Analysis** (`error_analyzer.py`): Rule-based + ML classifier, deficiency detection
- **API Endpoints** (`main.py`): RESTful service for test generation and classification

### Frontend (HTML5 + JavaScript)
- Image upload with drag-and-drop
- Interactive patch-ordering interface
- Results visualization with confidence bars
- Multi-step workflow UI

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.8+
- pip or conda

### Setup

1. **Clone or navigate to the project**:
   ```bash
   cd color-vision-ai
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

## üéØ Quick Start

### Start the Backend Server

```bash
cd backend
python main.py
```

The API will be available at `http://localhost:8000`.

**API Documentation** (auto-generated by FastAPI):
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Open the Frontend

```bash
# In a new terminal, navigate to frontend directory
cd frontend
# Use any local server (e.g., Python's built-in)
python -m http.server 8080
```

Then open `http://localhost:8080` in your browser.

**Workflow**:
1. Upload an image (landscape, artwork, product photo, etc.)
2. View extracted dominant colors
3. Drag and drop colors to arrange them in perceived order
4. Submit your response
5. Get classification result + recommendations

## üì° API Endpoints

### 1. Upload Image and Generate Test
```http
POST /upload-image
Content-Type: multipart/form-data

Body: { file: <image_file> }

Response:
{
  "session_id": "session_0",
  "test_spec": {
    "test_type": "patch_ordering",
    "n_colors": 12,
    "reference_order": [0, 1, 2, ...],
    "user_test_order": [5, 2, 8, ...],
    "reference_colors_rgb": [[255, 0, 0], ...],
    "reference_colors_lab": [[50, 20, 30], ...],
    "patch_configs": [...]
  }
}
```

### 2. Submit User Response
```http
POST /submit-response?session_id=session_0&user_order=[2, 5, 1, ...]
Content-Type: application/json

Response:
{
  "classification": {
    "predicted_class": "Deutan",
    "confidence": 0.87,
    "class_probabilities": {
      "Normal": 0.05,
      "Protan": 0.02,
      "Deutan": 0.87,
      "Tritan": 0.03,
      "Display Calibration Issue": 0.03
    },
    "deficiency_scores": {
      "protan": 0.1,
      "deutan": 0.8,
      "tritan": 0.15,
      "normal": 0.05,
      "display_calibration": 0.1
    }
  },
  "accuracy_score": 72.5,
  "report": {
    "color_vision_type": "Deutan",
    "severity": "Moderate",
    "accuracy_score": 72.5,
    "confidence": 0.87,
    "recommendation": "Green color vision deficiency detected..."
  }
}
```

### 3. Generate Distractor Test (Harder)
```http
POST /generate-distractor-test?session_id=session_0
Response: { "test_type": "distractor_matching", "groups": [...] }
```

### 4. Generate Luminance-Equalized Test
```http
POST /generate-luminance-test?session_id=session_0
Response: { "test_type": "luminance_equalized", "reference_colors_lab": [...] }
```

### 5. Get Session Details
```http
GET /session/session_0
Response:
{
  "session_id": "session_0",
  "has_response": true,
  "classification": {...},
  "accuracy_score": 72.5,
  "report": {...}
}
```

## üß™ Testing

Run unit tests for all core modules:

```bash
cd tests
pytest test_backend.py -v
```

**Test Coverage**:
- ‚úÖ RGB ‚Üî LAB color space conversions
- ‚úÖ Dominant color extraction
- ‚úÖ Test generation (patch, distractor, luminance)
- ‚úÖ Error metrics computation
- ‚úÖ Deficiency classification
- ‚úÖ End-to-end workflow

## üìä Algorithm Details

### 1. Color Extraction
- **Input**: RGB image
- **Process**:
  1. Resize image for efficiency (if needed)
  2. Apply K-Means clustering (k=12 dominant colors)
  3. Convert cluster centers from RGB to CIE LAB
- **Output**: 12 dominant colors in LAB space

### 2. Color Space (CIE LAB)
- **L**: Luminance (0‚Äì100)
- **a**: Red-Green axis (‚àí127 to +127)
- **b**: Yellow-Blue axis (‚àí127 to +127)
- **Advantage**: Perceptually uniform ‚Üí errors in LAB better correlate to human perception

### 3. Test Generation
- **Patch Ordering**: Shuffle colors, ask user to arrange in perceived order
- **Distractor Matching**: Add similar colors as distractors for harder test
- **Luminance-Equalized**: Normalize luminance to isolate hue-saturation discrimination

### 4. Error Analysis

**Error Metrics**:
- **Position Error**: How far each color ended up from correct position
- **Color Difference (ŒîE)**: Euclidean distance in LAB space
- **Channel Errors**: Separate L, a, b channel contributions

**Deficiency Scoring** (Rule-Based):
- **Protan/Deutan**: High errors on **a** channel (red-green axis)
- **Tritan**: High errors on **b** channel (yellow-blue axis)
- **Display Calibration**: High **L** errors, low **a/b** errors
- **Normal**: Low overall errors

**Classification** (ML):
- Features: [protan_score, deutan_score, tritan_score, calibration_score]
- Classifier: Random Forest (trained on synthetic patterns, upgradeable with real data)
- Output: Predicted class + confidence scores

### 5. Accuracy Score
```
Total Error = (position_error + color_error) / 2
Accuracy = 100 √ó (1 ‚àí total_error)  [0‚Äì100%]
```

## üéì Example Workflow

### Input: Sunset Image
```
1. User uploads sunset_photo.jpg
2. System extracts 12 dominant colors:
   - Orange, Red, Yellow, Pink, Purple, etc.
3. Generates test: "Arrange these 12 colors in perceived order"
4. User arranges them: sees shifts toward red/green or blue
5. System detects: "Deutan Deficiency (Green weak), Moderate severity, 68% accuracy"
6. Recommendation: Use color-blind friendly palettes, consider vision correction filters
```

## üìà Roadmap

### Phase 2: Adaptive Learning
- [ ] Collect user response data over time
- [ ] Retrain classifier with real demographic data
- [ ] Implement Bayesian adaptive testing (next test tailored to user errors)

### Phase 3: Extended Diagnostics
- [ ] Add severity classification (mild, moderate, severe)
- [ ] Support multiple test types (D-15, Ishihara simulation)
- [ ] Integration with professional color management tools

### Phase 4: Deployment
- [ ] Docker containerization
- [ ] Cloud deployment (AWS, Azure, GCP)
- [ ] Mobile app (React Native / Flutter)
- [ ] Integration with healthcare systems (HIPAA compliance)

## üîß Configuration

### Color Extraction
Edit `backend/color_extractor.py`:
```python
color_extractor = ColorExtractor(n_colors=12)  # Adjust number of dominant colors
```

### Test Generation
Edit `backend/test_generator.py`:
```python
test_generator = TestGenerator(test_type="patch_ordering")  # Or "matching"
```

### Classifier
Edit `backend/error_analyzer.py`:
- Modify scoring functions to adjust sensitivity
- Retrain with custom dataset using `_init_classifier()`

## üêõ Troubleshooting

### CORS Errors
- Ensure backend is running with CORS enabled (FastAPI middleware configured)
- Check that frontend is accessing `http://localhost:8000`

### Color Conversion Issues
- Verify image is in RGB format (not RGBA or grayscale)
- Check LAB conversion bounds: L [0, 100], a/b [‚àí127, 127]

### Low Classification Confidence
- Use images with diverse color palettes (landscapes, artwork)
- Check that user arranged at least 50% of colors correctly

## üìö References

- **CIE LAB Color Space**: [Wikipedia](https://en.wikipedia.org/wiki/CIELAB_color_space)
- **Color Blindness Types**: [Ishihara Test](https://en.wikipedia.org/wiki/Ishihara_test), [D-15 Test](https://en.wikipedia.org/wiki/D-15_color_test)
- **Human Color Vision**: [Photopigment Absorption Spectra](https://en.wikipedia.org/wiki/Photopigment)
- **ŒîE Color Difference**: [CIE ŒîE 2000](https://en.wikipedia.org/wiki/Color_difference)

## üìÑ License

MIT License ‚Äî Free to use, modify, and distribute.

## ü§ù Contributing

Contributions welcome! Areas for improvement:
- Real user data for classifier training
- Advanced error pattern analysis
- Mobile app development
- Accessibility improvements

## ‚úâÔ∏è Contact & Support

For questions, issues, or suggestions:
- Open an issue on GitHub
- Contribute test data for improved accuracy
- Share feedback on UI/UX

---

**Made with ‚ù§Ô∏è for accessible color vision diagnostics**
